{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jafarid/code/yogaposes/research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jafarid/code/yogaposes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path \n",
    "    resnet_trained_model_path: Path\n",
    "    resnet_updated_base_model_path: Path\n",
    "    traning_data: Path\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list \n",
    "    params_batch_size: int \n",
    "    params_epoches: int\n",
    "    params_learning_rate: float\n",
    "    all_params: dict\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yogaposes.constants import *\n",
    "from yogaposes.utils.common import read_yaml, create_directories\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.environ['MLFLOW_TRACKING_URI']\n",
    "MLFLOW_TRACKING_USERNAME = os.environ['MLFLOW_TRACKING_USERNAME']\n",
    "MLFLOW_TRACKING_PASSWORD = os.environ['MLFLOW_TRACKING_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "    def __init__(self,config_file_path=CONFIG_FILE_PATH, params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_traning_config(self) -> TrainingConfig:\n",
    "        model_training = self.config.model_training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        training_data = os.path.join(self.config.data_ingestion.root_dir, 'yoga-poses-dataset')\n",
    "        \n",
    "        create_directories([model_training.root_dir])\n",
    "        \n",
    "        training_config = TrainingConfig(root_dir= model_training.resnet_trained_model_path, \n",
    "                                        resnet_trained_model_path= model_training.resnet_trained_model_path,\n",
    "                                        resnet_updated_base_model_path= prepare_base_model.resnet_updated_base_model_path,\n",
    "                                        traning_data = training_data,\n",
    "                                        params_augmentation = self.params.AUGMENTATION,\n",
    "                                        params_image_size = self.params.IMAGE_SIZE,\n",
    "                                        params_batch_size= self.params.BATCH_SIZE,\n",
    "                                        params_epoches = self.params.EPOCHS,\n",
    "                                        params_learning_rate = self.params.LEARNING_RATE,\n",
    "                                        all_params = self.params,\n",
    "                                        mlflow_uri= MLFLOW_TRACKING_URI\n",
    "                                        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "from PIL import ImageFile, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    def __init__(self, config:TrainingConfig, loss_fn=None, optimizer=None):\n",
    "        self.config = config\n",
    "        self.model = self.load_model()\n",
    "        self.loss_fn = loss_fn if loss_fn else nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader, self.val_loader = self.set_loaders()\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.total_epoches = 0\n",
    "        \n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "        \n",
    "    def load_model(self):\n",
    "     \n",
    "        return torch.load(self.config.resnet_updated_base_model_path)\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_loaders(self):\n",
    "        \n",
    "        # Allow loading of truncated images\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        \n",
    "        # image net statistics\n",
    "        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "        \n",
    "        composer = Compose([Resize(256), CenterCrop(224), ToTensor(), normalizer])\n",
    "        \n",
    "        train_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TRAIN'), transform=composer)\n",
    "        val_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TEST'), transform=composer)\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.config.params_batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    # higher order function to be set and built globally and constructed the inner fuction without knowning x and y before hand\n",
    "    def _make_train_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_train_step_fn(x,y):\n",
    "            # set the train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # step 1: compute model output\n",
    "            yhat = self.model(x)\n",
    "            #yhat = yhat.float()\n",
    "            #y = y.float()\n",
    "            # step 2: compute the loss\n",
    "              \n",
    "            loss= self.loss_fn(yhat,y)\n",
    "            \n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "            \n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            # step 3: compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            #step4: update parameters\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            #step 5: return the loss\n",
    "            return loss.item() , acc\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_val_step_fn(x,y):\n",
    "            # set the model in val mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            #step 1: compute the prediction\n",
    "            yhat = self.model(x)\n",
    "            #yhat = yhat.float()\n",
    "            #y = y.float()\n",
    "            #step 2: compute the loss\n",
    "            loss = self.loss_fn(yhat,y)\n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "          \n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            return loss.item(), acc\n",
    "        return perform_val_step_fn\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # one epoch operation \n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "            \n",
    "        else: \n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "            \n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        mini_batch_accs = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            mini_batch_loss, mini_batch_acc = step_fn(x_batch,y_batch)\n",
    "            \n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "            mini_batch_accs.append(mini_batch_acc)\n",
    "        \n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        acc = np.mean(mini_batch_accs)\n",
    "        return loss, acc\n",
    "    \n",
    "    def train(self, seed=42):\n",
    "        self.set_seed(seed)\n",
    "        \n",
    "        for epoch in range(self.config.params_epoches):\n",
    "            self.total_epoches +=1\n",
    "            \n",
    "            # perform training on mini batches within 1 epoch\n",
    "            loss, acc = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "            self.accuracy.append(acc)\n",
    "            # now calc validation\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracy.append(val_acc)\n",
    "        \n",
    "            print(f'\\nEpoch: {epoch+1} \\tTraining Loss: {loss:.4f} \\tValidation Loss: {val_loss:.4f}')\n",
    "            print(f'\\t\\tTraining Accuracy: {100 * acc:.2f}%\\t Validation Accuracy: {100 * val_acc:.2f}%')\n",
    "                \n",
    "        self.save_checkpoint()\n",
    "            \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = {'epoch': self.total_epoches,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'accuracy': self.accuracy,\n",
    "                      'val_loss': self.val_losses,\n",
    "                      'val_accuracy': self.val_accuracy\n",
    "                      }\n",
    "        torch.save(checkpoint, self.config.resnet_trained_model_path)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.config.resnet_trained_model_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.total_epoches = checkpoint[\"epoch\"]\n",
    "        self.losses = checkpoint[\"loss\"]\n",
    "        self.accuracy = checkpoint['accuracy']\n",
    "        self.val_accuracy = checkpoint['val_accuracy']\n",
    "        self.val_losses = checkpoint[\"val_loss\"]\n",
    "        self.model.train() # always use train for resuming traning\n",
    "        \n",
    "    def _preprocess_image(self, filename):\n",
    "        image = Image.open(filename)\n",
    "        # Allow loading of truncated images\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        \n",
    "        # image net statistics\n",
    "        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "        \n",
    "        composer = Compose([Resize(256), CenterCrop(224), ToTensor(), normalizer])\n",
    "        image = composer(image).unsqueeze(0)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    def predict(self,x_filename):\n",
    "        \n",
    "        self.load_checkpoint()\n",
    "        self.model.eval()\n",
    "        x = self._preprocess_image(x_filename)\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        \n",
    "        # set it back to the train mode\n",
    "        self.model.train()\n",
    "        \n",
    "        labels = {0:'downdog', 1: 'godess', 2:'plank', 3:'tree', 4:'warrior2'}\n",
    "        prediction=np.argmax(y_hat_tensor.detach().cpu().numpy())\n",
    "        \n",
    "        return labels[prediction]\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics({'train_loss': np.mean(self.losses),'val_loss': np.mean(self.val_losses), 'train_accuracy': np.mean(self.accuracy), 'val_accuracy': np.mean(self.val_accuracy)})\n",
    "        \n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=\"ResNet18Model\")\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 02:03:18,404: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-05 02:03:18,406: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-05 02:03:18,407: INFO: common: created directory at: artifacts]\n",
      "[2024-03-05 02:03:18,407: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jafarid/miniconda3/envs/yogaposes/lib/python3.8/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \tTraining Loss: 8.5584 \tValidation Loss: 4.1259\n",
      "\t\tTraining Accuracy: 53.55%\t Validation Accuracy: 79.24%\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 3.5759 \tValidation Loss: 1.1918\n",
      "\t\tTraining Accuracy: 70.33%\t Validation Accuracy: 89.17%\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 2.8397 \tValidation Loss: 4.4006\n",
      "\t\tTraining Accuracy: 76.90%\t Validation Accuracy: 75.00%\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 2.3950 \tValidation Loss: 1.7030\n",
      "\t\tTraining Accuracy: 79.78%\t Validation Accuracy: 88.12%\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 2.6101 \tValidation Loss: 1.0900\n",
      "\t\tTraining Accuracy: 79.40%\t Validation Accuracy: 90.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ResNet18Model' already exists. Creating a new version of this model...\n",
      "2024/03/05 02:06:05 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ResNet18Model, version 3\n",
      "Created version '3' of model 'ResNet18Model'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_traning_config()\n",
    "    training = ModelTrainer(config=training_config)\n",
    "    training.train()\n",
    "    training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_traning_config()\n",
    "    training = ModelTrainer(config=training_config, inference=True)\n",
    "    training.train()\n",
    "    training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 19:20:36,074: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-05 19:20:36,076: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-05 19:20:36,077: INFO: common: created directory at: artifacts]\n",
      "[2024-03-05 19:20:36,077: INFO: common: created directory at: artifacts/training]\n",
      "plank\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_inference = configurationManager()\n",
    "    inference_config = config_inference.get_traning_config()\n",
    "    inference = ModelTrainer(config=inference_config)\n",
    "    c = inference.predict('artifacts/data_ingestion/yoga-poses-dataset/DATASET/TRAIN/plank/00000128.jpg')\n",
    "    print(c)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yogaposes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
