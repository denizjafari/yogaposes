{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jafarid/code/yogaposes/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jafarid/code/yogaposes'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path \n",
    "    resnet_trained_model_path: Path\n",
    "    resnet_updated_base_model_path: Path\n",
    "    traning_data: Path\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list \n",
    "    params_batch_size: int \n",
    "    params_epoches: int\n",
    "    params_learning_rate: float\n",
    "    params_classes: int\n",
    "    params_type: str\n",
    "    all_params: dict\n",
    "    mlflow_uri: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yogaposes.constants import *\n",
    "from yogaposes.utils.common import read_yaml, create_directories\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.environ['MLFLOW_TRACKING_URI']\n",
    "MLFLOW_TRACKING_USERNAME = os.environ['MLFLOW_TRACKING_USERNAME']\n",
    "MLFLOW_TRACKING_PASSWORD = os.environ['MLFLOW_TRACKING_PASSWORD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "    def __init__(self,config_file_path=CONFIG_FILE_PATH, params_file_path=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_traning_config(self) -> TrainingConfig:\n",
    "        model_training = self.config.model_training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        training_data = os.path.join(self.config.data_ingestion.root_dir, 'yoga-poses-dataset')\n",
    "        \n",
    "        create_directories([model_training.root_dir])\n",
    "        \n",
    "        training_config = TrainingConfig(root_dir= model_training.resnet_trained_model_path, \n",
    "                                        resnet_trained_model_path= model_training.resnet_trained_model_path,\n",
    "                                        resnet_updated_base_model_path= prepare_base_model.resnet_updated_base_model_path,\n",
    "                                        traning_data = training_data,\n",
    "                                        params_augmentation = self.params.AUGMENTATION,\n",
    "                                        params_image_size = self.params.IMAGE_SIZE,\n",
    "                                        params_classes = self.params.CLASSES,\n",
    "                                        params_batch_size= self.params.BATCH_SIZE,\n",
    "                                        params_epoches = self.params.EPOCHS,\n",
    "                                        params_learning_rate = self.params.LEARNING_RATE,\n",
    "                                        params_type = self.params.TYPE,\n",
    "                                        all_params = self.params,\n",
    "                                        mlflow_uri= MLFLOW_TRACKING_URI\n",
    "                                        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, CenterCrop\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "from PIL import ImageFile, Image\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTImageProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torchinfo import summary\n",
    "import requests\n",
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,              # Total number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelTrainer(object):\n",
    "    def __init__(self, config:TrainingConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        self.model, self.feature_extractor, self.processor = self.load_model()\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader, self.val_loader = self.set_loaders()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_model(self):\n",
    "        idTolabels = {0:'downdog', 1: 'godess', 2:'plank', 3:'tree', 4:'warrior2'}\n",
    "        feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "        processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=self.config.params_classes,ignore_mismatched_sizes=True,id2label=idTolabels)\n",
    "        \n",
    "        return model, feature_extractor , processor\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_loaders(self):\n",
    "        \n",
    "        # Allow loading of truncated images\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        \n",
    "        # image net statistics\n",
    "        normalizer = Normalize(mean=self.feature_extractor.image_mean, std=self.feature_extractor.image_std)\n",
    "        \n",
    "        composer = Compose([Resize((224,224)), ToTensor(), normalizer])\n",
    "        \n",
    "        train_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TRAIN'), transform=composer)\n",
    "        val_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TEST'), transform=composer)\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.config.params_batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def collate_fn(self, examples):\n",
    "        pixels = torch.stack([example[\"pixels\"] for example in examples])\n",
    "        labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "        return {\"pixel_values\": pixels, \"labels\": labels}\n",
    "\n",
    "    def compute_metrics(self, eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return dict(accuracy=accuracy_score(predictions, labels))\n",
    "    \n",
    "    def fine_tune(self, training_args):\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        \n",
    "        # image net statistics\n",
    "        normalizer = Normalize(mean=self.feature_extractor.image_mean, std=self.feature_extractor.image_std)\n",
    "        \n",
    "        composer = Compose([Resize((224,224)), ToTensor(), normalizer])\n",
    "        train_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TRAIN'), transform=composer)\n",
    "        val_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TEST'), transform=composer)\n",
    "        trainer = Trainer(model=self.model,\n",
    "                            args=training_args,\n",
    "                            train_dataset=train_data,\n",
    "                            eval_dataset=val_data,\n",
    "                            #tokenizer=self.feature_extractor,\n",
    "                            data_collator = self.set_loaders,\n",
    "                            tokenizer = self.processor,\n",
    "                        )\n",
    "        trainer.train()\n",
    "        \n",
    "\n",
    "    def predict(self,x_filename):\n",
    "        \n",
    "        # Allow loading of truncated images\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True       \n",
    "        # Load an image\n",
    "        image = Image.open(x_filename)\n",
    "        encoding = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoding)\n",
    "\n",
    "        # Extract the predicted class ID\n",
    "        predicted_class_id = outputs.logits.argmax(-1).item()\n",
    "       \n",
    "        \n",
    "        labels = {0:'downdog', 1: 'godess', 2:'plank', 3:'tree', 4:'warrior2'}\n",
    "    \n",
    "        \n",
    "        return labels[predicted_class_id]\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-14 20:09:09,950: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-14 20:09:09,952: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-14 20:09:09,953: INFO: common: created directory at: artifacts]\n",
      "[2024-03-14 20:09:09,953: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/408 [05:26<?, ?it/s]\n",
      "  0%|          | 0/408 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "set_loaders() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(c)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32m/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m training_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_traning_config()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m training \u001b[39m=\u001b[39m TransformerModelTrainer(config\u001b[39m=\u001b[39mtraining_config)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m training\u001b[39m.\u001b[39;49mfine_tune(training_args)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m c \u001b[39m=\u001b[39m training\u001b[39m.\u001b[39mpredict(\u001b[39m'\u001b[39m\u001b[39martifacts/data_ingestion/yoga-poses-dataset/DATASET/TRAIN/plank/00000128.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(c)\n",
      "\u001b[1;32m/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m val_data \u001b[39m=\u001b[39m ImageFolder(root\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtraning_data,\u001b[39m'\u001b[39m\u001b[39mDATASET/TEST\u001b[39m\u001b[39m'\u001b[39m), transform\u001b[39m=\u001b[39mcomposer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m                     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m                     train_dataset\u001b[39m=\u001b[39mtrain_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m                     tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m                 )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/03_model_training_transformer.ipynb#X23sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1625\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1626\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1627\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1628\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1629\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/transformers/trainer.py:1928\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1925\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1928\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1929\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1931\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[1;32m    453\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/transformers/trainer_utils.py:773\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[\u001b[39mdict\u001b[39m]):\n\u001b[1;32m    772\u001b[0m     features \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remove_columns(feature) \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m features]\n\u001b[0;32m--> 773\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_collator(features)\n",
      "\u001b[0;31mTypeError\u001b[0m: set_loaders() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_traning_config()\n",
    "    training = TransformerModelTrainer(config=training_config)\n",
    "    training.fine_tune(training_args)\n",
    "    c = training.predict('artifacts/data_ingestion/yoga-poses-dataset/DATASET/TRAIN/plank/00000128.jpg')\n",
    "    print(c)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    def __init__(self, config:TrainingConfig, loss_fn=None, optimizer=None):\n",
    "        self.config = config\n",
    "        self.model = self.load_model()\n",
    "        self.loss_fn = loss_fn if loss_fn else nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader, self.val_loader = self.set_loaders()\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.total_epoches = 0\n",
    "        \n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "        \n",
    "    def load_model(self):\n",
    "     \n",
    "        return torch.load(self.config.resnet_updated_base_model_path)\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_loaders(self):\n",
    "        \n",
    "        # Allow loading of truncated images\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        \n",
    "        # image net statistics\n",
    "        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "        \n",
    "        composer = Compose([Resize(256), CenterCrop(224), ToTensor(), normalizer])\n",
    "        \n",
    "        train_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TRAIN'), transform=composer)\n",
    "        val_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TEST'), transform=composer)\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.config.params_batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    # higher order function to be set and built globally and constructed the inner fuction without knowning x and y before hand\n",
    "    def _make_train_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_train_step_fn(x,y):\n",
    "            # set the train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # step 1: compute model output\n",
    "            yhat = self.model(x)\n",
    "            #yhat = yhat.float()\n",
    "            #y = y.float()\n",
    "            # step 2: compute the loss\n",
    "              \n",
    "            loss= self.loss_fn(yhat,y)\n",
    "            \n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "            \n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            # step 3: compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            #step4: update parameters\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            #step 5: return the loss\n",
    "            return loss.item() , acc\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_val_step_fn(x,y):\n",
    "            # set the model in val mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            #step 1: compute the prediction\n",
    "            yhat = self.model(x)\n",
    "            #yhat = yhat.float()\n",
    "            #y = y.float()\n",
    "            #step 2: compute the loss\n",
    "            loss = self.loss_fn(yhat,y)\n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "          \n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            return loss.item(), acc\n",
    "        return perform_val_step_fn\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # one epoch operation \n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "            \n",
    "        else: \n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "            \n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        mini_batch_accs = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            mini_batch_loss, mini_batch_acc = step_fn(x_batch,y_batch)\n",
    "            \n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "            mini_batch_accs.append(mini_batch_acc)\n",
    "        \n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        acc = np.mean(mini_batch_accs)\n",
    "        return loss, acc\n",
    "    \n",
    "    def train(self, seed=42):\n",
    "        self.set_seed(seed)\n",
    "        \n",
    "        for epoch in range(self.config.params_epoches):\n",
    "            self.total_epoches +=1\n",
    "            \n",
    "            # perform training on mini batches within 1 epoch\n",
    "            loss, acc = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "            self.accuracy.append(acc)\n",
    "            # now calc validation\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracy.append(val_acc)\n",
    "        \n",
    "            print(f'\\nEpoch: {epoch+1} \\tTraining Loss: {loss:.4f} \\tValidation Loss: {val_loss:.4f}')\n",
    "            print(f'\\t\\tTraining Accuracy: {100 * acc:.2f}%\\t Validation Accuracy: {100 * val_acc:.2f}%')\n",
    "                \n",
    "        self.save_checkpoint()\n",
    "            \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = {'epoch': self.total_epoches,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'accuracy': self.accuracy,\n",
    "                      'val_loss': self.val_losses,\n",
    "                      'val_accuracy': self.val_accuracy\n",
    "                      }\n",
    "        torch.save(checkpoint, self.config.resnet_trained_model_path)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.config.resnet_trained_model_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.total_epoches = checkpoint[\"epoch\"]\n",
    "        self.losses = checkpoint[\"loss\"]\n",
    "        self.accuracy = checkpoint['accuracy']\n",
    "        self.val_accuracy = checkpoint['val_accuracy']\n",
    "        self.val_losses = checkpoint[\"val_loss\"]\n",
    "        self.model.train() # always use train for resuming traning\n",
    "        \n",
    "    def _preprocess_image(self, filename):\n",
    "        image = Image.open(filename)\n",
    "        # Allow loading of truncated images\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        \n",
    "        # image net statistics\n",
    "        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "        \n",
    "        composer = Compose([Resize(256), CenterCrop(224), ToTensor(), normalizer])\n",
    "        image = composer(image).unsqueeze(0)\n",
    "        return image\n",
    "        \n",
    "    \n",
    "    def predict(self,x_filename):\n",
    "        \n",
    "        self.load_checkpoint()\n",
    "        self.model.eval()\n",
    "        x = self._preprocess_image(x_filename)\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        \n",
    "        # set it back to the train mode\n",
    "        self.model.train()\n",
    "        \n",
    "        labels = {0:'downdog', 1: 'godess', 2:'plank', 3:'tree', 4:'warrior2'}\n",
    "        prediction=np.argmax(y_hat_tensor.detach().cpu().numpy())\n",
    "        \n",
    "        return labels[prediction]\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics({'train_loss': np.mean(self.losses),'val_loss': np.mean(self.val_losses), 'train_accuracy': np.mean(self.accuracy), 'val_accuracy': np.mean(self.val_accuracy)})\n",
    "        \n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=\"ResNet18Model\")\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 02:03:18,404: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-05 02:03:18,406: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-05 02:03:18,407: INFO: common: created directory at: artifacts]\n",
      "[2024-03-05 02:03:18,407: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jafarid/miniconda3/envs/yogaposes/lib/python3.8/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \tTraining Loss: 8.5584 \tValidation Loss: 4.1259\n",
      "\t\tTraining Accuracy: 53.55%\t Validation Accuracy: 79.24%\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 3.5759 \tValidation Loss: 1.1918\n",
      "\t\tTraining Accuracy: 70.33%\t Validation Accuracy: 89.17%\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 2.8397 \tValidation Loss: 4.4006\n",
      "\t\tTraining Accuracy: 76.90%\t Validation Accuracy: 75.00%\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 2.3950 \tValidation Loss: 1.7030\n",
      "\t\tTraining Accuracy: 79.78%\t Validation Accuracy: 88.12%\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 2.6101 \tValidation Loss: 1.0900\n",
      "\t\tTraining Accuracy: 79.40%\t Validation Accuracy: 90.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'ResNet18Model' already exists. Creating a new version of this model...\n",
      "2024/03/05 02:06:05 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: ResNet18Model, version 3\n",
      "Created version '3' of model 'ResNet18Model'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_traning_config()\n",
    "    training = ModelTrainer(config=training_config)\n",
    "    training.train()\n",
    "    training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_traning_config()\n",
    "    training = ModelTrainer(config=training_config, inference=True)\n",
    "    training.train()\n",
    "    training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 19:20:36,074: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-05 19:20:36,076: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-05 19:20:36,077: INFO: common: created directory at: artifacts]\n",
      "[2024-03-05 19:20:36,077: INFO: common: created directory at: artifacts/training]\n",
      "plank\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_inference = configurationManager()\n",
    "    inference_config = config_inference.get_traning_config()\n",
    "    inference = ModelTrainer(config=inference_config)\n",
    "    c = inference.predict('artifacts/data_ingestion/yoga-poses-dataset/DATASET/TRAIN/plank/00000128.jpg')\n",
    "    print(c)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yogaposes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
