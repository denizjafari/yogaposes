{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jafarid/code/yogaposes'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    root_dir: Path\n",
    "    resnet_base_model_path: Path\n",
    "    resnet_updated_base_model_path: Path\n",
    "    params_class: int\n",
    "    params_image_size: list\n",
    "    params_pretrained: bool\n",
    "    params_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yogaposes.constants import *\n",
    "from yogaposes.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager: \n",
    "    def __init__(self, config_file_path= CONFIG_FILE_PATH, params_file_path= PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_prepare_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.prepare_base_model\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        get_prepare_base_model_config = PrepareBaseModelConfig(root_dir= config.root_dir,\n",
    "                                                               resnet_base_model_path=config.resnet_base_model_path,\n",
    "                                                               resnet_updated_base_model_path= config.resnet_updated_base_model_path,\n",
    "                                                               params_class=self.params.CLASSES,\n",
    "                                                               params_image_size= self.params.IMAGE_SIZE,\n",
    "                                                               params_pretrained = self.params.PRETRAINED,\n",
    "                                                               params_type= self.params.TYPE\n",
    "                                                               )\n",
    "\n",
    "        return get_prepare_base_model_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import models\n",
    "import torch \n",
    "from torchsummary import summary\n",
    "from yogaposes import logger\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jafarid/miniconda3/envs/yogaposes/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load feature extractor and model\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "vit.embeddings.cls_token: 768\n",
      "vit.embeddings.position_embeddings: 151296\n",
      "vit.embeddings.patch_embeddings.projection.weight: 589824\n",
      "vit.embeddings.patch_embeddings.projection.bias: 768\n",
      "vit.encoder.layer.0.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.0.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.0.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.0.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.0.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.0.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.0.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.0.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.0.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.0.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.0.output.dense.weight: 2359296\n",
      "vit.encoder.layer.0.output.dense.bias: 768\n",
      "vit.encoder.layer.0.layernorm_before.weight: 768\n",
      "vit.encoder.layer.0.layernorm_before.bias: 768\n",
      "vit.encoder.layer.0.layernorm_after.weight: 768\n",
      "vit.encoder.layer.0.layernorm_after.bias: 768\n",
      "vit.encoder.layer.1.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.1.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.1.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.1.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.1.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.1.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.1.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.1.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.1.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.1.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.1.output.dense.weight: 2359296\n",
      "vit.encoder.layer.1.output.dense.bias: 768\n",
      "vit.encoder.layer.1.layernorm_before.weight: 768\n",
      "vit.encoder.layer.1.layernorm_before.bias: 768\n",
      "vit.encoder.layer.1.layernorm_after.weight: 768\n",
      "vit.encoder.layer.1.layernorm_after.bias: 768\n",
      "vit.encoder.layer.2.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.2.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.2.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.2.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.2.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.2.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.2.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.2.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.2.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.2.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.2.output.dense.weight: 2359296\n",
      "vit.encoder.layer.2.output.dense.bias: 768\n",
      "vit.encoder.layer.2.layernorm_before.weight: 768\n",
      "vit.encoder.layer.2.layernorm_before.bias: 768\n",
      "vit.encoder.layer.2.layernorm_after.weight: 768\n",
      "vit.encoder.layer.2.layernorm_after.bias: 768\n",
      "vit.encoder.layer.3.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.3.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.3.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.3.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.3.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.3.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.3.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.3.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.3.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.3.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.3.output.dense.weight: 2359296\n",
      "vit.encoder.layer.3.output.dense.bias: 768\n",
      "vit.encoder.layer.3.layernorm_before.weight: 768\n",
      "vit.encoder.layer.3.layernorm_before.bias: 768\n",
      "vit.encoder.layer.3.layernorm_after.weight: 768\n",
      "vit.encoder.layer.3.layernorm_after.bias: 768\n",
      "vit.encoder.layer.4.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.4.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.4.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.4.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.4.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.4.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.4.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.4.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.4.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.4.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.4.output.dense.weight: 2359296\n",
      "vit.encoder.layer.4.output.dense.bias: 768\n",
      "vit.encoder.layer.4.layernorm_before.weight: 768\n",
      "vit.encoder.layer.4.layernorm_before.bias: 768\n",
      "vit.encoder.layer.4.layernorm_after.weight: 768\n",
      "vit.encoder.layer.4.layernorm_after.bias: 768\n",
      "vit.encoder.layer.5.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.5.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.5.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.5.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.5.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.5.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.5.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.5.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.5.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.5.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.5.output.dense.weight: 2359296\n",
      "vit.encoder.layer.5.output.dense.bias: 768\n",
      "vit.encoder.layer.5.layernorm_before.weight: 768\n",
      "vit.encoder.layer.5.layernorm_before.bias: 768\n",
      "vit.encoder.layer.5.layernorm_after.weight: 768\n",
      "vit.encoder.layer.5.layernorm_after.bias: 768\n",
      "vit.encoder.layer.6.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.6.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.6.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.6.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.6.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.6.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.6.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.6.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.6.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.6.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.6.output.dense.weight: 2359296\n",
      "vit.encoder.layer.6.output.dense.bias: 768\n",
      "vit.encoder.layer.6.layernorm_before.weight: 768\n",
      "vit.encoder.layer.6.layernorm_before.bias: 768\n",
      "vit.encoder.layer.6.layernorm_after.weight: 768\n",
      "vit.encoder.layer.6.layernorm_after.bias: 768\n",
      "vit.encoder.layer.7.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.7.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.7.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.7.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.7.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.7.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.7.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.7.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.7.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.7.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.7.output.dense.weight: 2359296\n",
      "vit.encoder.layer.7.output.dense.bias: 768\n",
      "vit.encoder.layer.7.layernorm_before.weight: 768\n",
      "vit.encoder.layer.7.layernorm_before.bias: 768\n",
      "vit.encoder.layer.7.layernorm_after.weight: 768\n",
      "vit.encoder.layer.7.layernorm_after.bias: 768\n",
      "vit.encoder.layer.8.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.8.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.8.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.8.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.8.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.8.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.8.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.8.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.8.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.8.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.8.output.dense.weight: 2359296\n",
      "vit.encoder.layer.8.output.dense.bias: 768\n",
      "vit.encoder.layer.8.layernorm_before.weight: 768\n",
      "vit.encoder.layer.8.layernorm_before.bias: 768\n",
      "vit.encoder.layer.8.layernorm_after.weight: 768\n",
      "vit.encoder.layer.8.layernorm_after.bias: 768\n",
      "vit.encoder.layer.9.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.9.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.9.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.9.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.9.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.9.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.9.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.9.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.9.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.9.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.9.output.dense.weight: 2359296\n",
      "vit.encoder.layer.9.output.dense.bias: 768\n",
      "vit.encoder.layer.9.layernorm_before.weight: 768\n",
      "vit.encoder.layer.9.layernorm_before.bias: 768\n",
      "vit.encoder.layer.9.layernorm_after.weight: 768\n",
      "vit.encoder.layer.9.layernorm_after.bias: 768\n",
      "vit.encoder.layer.10.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.10.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.10.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.10.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.10.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.10.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.10.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.10.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.10.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.10.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.10.output.dense.weight: 2359296\n",
      "vit.encoder.layer.10.output.dense.bias: 768\n",
      "vit.encoder.layer.10.layernorm_before.weight: 768\n",
      "vit.encoder.layer.10.layernorm_before.bias: 768\n",
      "vit.encoder.layer.10.layernorm_after.weight: 768\n",
      "vit.encoder.layer.10.layernorm_after.bias: 768\n",
      "vit.encoder.layer.11.attention.attention.query.weight: 589824\n",
      "vit.encoder.layer.11.attention.attention.query.bias: 768\n",
      "vit.encoder.layer.11.attention.attention.key.weight: 589824\n",
      "vit.encoder.layer.11.attention.attention.key.bias: 768\n",
      "vit.encoder.layer.11.attention.attention.value.weight: 589824\n",
      "vit.encoder.layer.11.attention.attention.value.bias: 768\n",
      "vit.encoder.layer.11.attention.output.dense.weight: 589824\n",
      "vit.encoder.layer.11.attention.output.dense.bias: 768\n",
      "vit.encoder.layer.11.intermediate.dense.weight: 2359296\n",
      "vit.encoder.layer.11.intermediate.dense.bias: 3072\n",
      "vit.encoder.layer.11.output.dense.weight: 2359296\n",
      "vit.encoder.layer.11.output.dense.bias: 768\n",
      "vit.encoder.layer.11.layernorm_before.weight: 768\n",
      "vit.encoder.layer.11.layernorm_before.bias: 768\n",
      "vit.encoder.layer.11.layernorm_after.weight: 768\n",
      "vit.encoder.layer.11.layernorm_after.bias: 768\n",
      "vit.layernorm.weight: 768\n",
      "vit.layernorm.bias: 768\n",
      "classifier.weight: 768000\n",
      "classifier.bias: 1000\n",
      "Total Parameters: 86567656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "    print(\"Model Summary:\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name}: {param.numel()}\")\n",
    "        total_params += param.numel()\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "\n",
    "model_summary(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.out_features = 5\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier.out_features = 5\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ViTEmbeddings: 2, ViTPatchEmbeddings: 3, Conv2d: 4, Dropout: 3, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torchinfo/torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 295\u001b[0m     _ \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49mx, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:794\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    792\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 794\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvit(\n\u001b[1;32m    795\u001b[0m     pixel_values,\n\u001b[1;32m    796\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    797\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    798\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    799\u001b[0m     interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding,\n\u001b[1;32m    800\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    801\u001b[0m )\n\u001b[1;32m    803\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:577\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    573\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    574\u001b[0m     pixel_values, bool_masked_pos\u001b[39m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[39m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    575\u001b[0m )\n\u001b[0;32m--> 577\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    578\u001b[0m     embedding_output,\n\u001b[1;32m    579\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    580\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    581\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    582\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torch/nn/modules/module.py:1574\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[1;32m   1576\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torchsummary/torchsummary.py:26\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(output\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m     27\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m batch_size\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutput' object has no attribute 'size'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jafarid/code/yogaposes/research/02_prepare_base_model_transformer.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/02_prepare_base_model_transformer.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#print(model.config)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jafarid/code/yogaposes/research/02_prepare_base_model_transformer.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m summary(model, input_size\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m)) \n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torchinfo/torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[1;32m    220\u001b[0m x, correct_input_size \u001b[39m=\u001b[39m process_input(\n\u001b[1;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[1;32m    222\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m summary_list \u001b[39m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[39m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[1;32m    227\u001b[0m results \u001b[39m=\u001b[39m ModelStatistics(\n\u001b[1;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[1;32m    229\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/yogaposes/lib/python3.8/site-packages/torchinfo/torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    303\u001b[0m     executed_layers \u001b[39m=\u001b[39m [layer \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m summary_list \u001b[39mif\u001b[39;00m layer\u001b[39m.\u001b[39mexecuted]\n\u001b[0;32m--> 304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecuted layers up to: \u001b[39m\u001b[39m{\u001b[39;00mexecuted_layers\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ViTEmbeddings: 2, ViTPatchEmbeddings: 3, Conv2d: 4, Dropout: 3, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6, ViTLayer: 4, LayerNorm: 5, ViTAttention: 5, ViTSelfAttention: 6, Linear: 7, Linear: 7, Linear: 7, Dropout: 7, ViTSelfOutput: 6, Linear: 7, Dropout: 7, LayerNorm: 5, ViTIntermediate: 5, Linear: 6, GELUActivation: 6, ViTOutput: 5, Linear: 6, Dropout: 6]"
     ]
    }
   ],
   "source": [
    "\n",
    "def transform_image():\n",
    "    return Compose([\n",
    "        Resize((224, 224)), # Resize images to fit model input size\n",
    "        ToTensor(), # Convert images to tensors\n",
    "        Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std) # Normalize images\n",
    "    ])\n",
    "\n",
    "train_dataset = ImageFolder('path/to/train', transform=transform_image())\n",
    "val_dataset = ImageFolder('path/to/val', transform=transform_image())\n",
    "\n",
    "train_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TRAIN'), transform=composer)\n",
    "val_data = ImageFolder(root=os.path.join(self.config.traning_data,'DATASET/TEST'), transform=composer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareBaseModel:\n",
    "    def __init__(self, config: PrepareBaseModelConfig):\n",
    "        self.config = config\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "    def get_base_model(self):\n",
    "        resnet_model = models.resnet18(pretrained=self.config.params_pretrained)\n",
    "        resnet_model.to(self.device)\n",
    "        self.save_model(resnet_model, self.config.resnet_base_model_path)\n",
    "        return resnet_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def _prepare_full_model(model, classes, freeze_till, freeze_all=True):\n",
    "    # internal function that we don't want to run\n",
    "        if freeze_all:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        elif (freeze_till is not None) and (freeze_till>0):\n",
    "            for param in model.parameters()[:-freeze_till]:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        n_inputs = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(n_inputs, classes)\n",
    "        return model\n",
    "    \n",
    "    def update_base_model(self):\n",
    "        self.full_model = self._prepare_full_model(model= self.get_base_model(), classes=self.config.params_class, freeze_all=True, freeze_till=None)\n",
    "        self.full_model.to(self.device)\n",
    "        summary(self.full_model,input_size=tuple(self.config.params_image_size), device=self.device)\n",
    "        \n",
    "        self.save_model(checkpoint=self.full_model, path=self.config.resnet_updated_base_model_path)\n",
    "        logger.info(f'Saved updated model to {str(self.config.root_dir)}')\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(checkpoint:dict,path:Path):\n",
    "        torch.save(checkpoint, path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-02 17:54:21,305: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-02 17:54:21,306: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-02 17:54:21,307: INFO: common: created directory at: artifacts]\n",
      "[2024-03-02 17:54:21,307: INFO: common: created directory at: artifacts/prepare_base_model]\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                    [-1, 5]           2,565\n",
      "================================================================\n",
      "Total params: 11,179,077\n",
      "Trainable params: 2,565\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 106.00\n",
      "----------------------------------------------------------------\n",
      "[2024-03-02 17:54:21,520: INFO: 3407935668: Saved updated model to artifacts/prepare_base_model]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_base_model_config = config.get_prepare_base_model_config()\n",
    "    prepare_base_model = PrepareBaseModel(config=prepare_base_model_config)\n",
    "    prepare_base_model.update_base_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yogaposes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
